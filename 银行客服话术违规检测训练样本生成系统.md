# é“¶è¡Œå®¢æœè¯æœ¯è¿è§„æ£€æµ‹è®­ç»ƒæ ·æœ¬ç”Ÿæˆç³»ç»Ÿ

## ğŸ“‹ ç›®å½•

- [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
- [æŠ€æœ¯æ ˆ](#æŠ€æœ¯æ ˆ)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [è¯¦ç»†æ–‡æ¡£](#è¯¦ç»†æ–‡æ¡£)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [æ‰©å±•å¼€å‘](#æ‰©å±•å¼€å‘)

---

## ğŸ¯ ç³»ç»Ÿæ¦‚è¿°

æœ¬ç³»ç»Ÿæ˜¯ä¸€ä¸ª**æ™ºèƒ½åŒ–çš„è®­ç»ƒæ•°æ®ç”Ÿæˆå¹³å°**ï¼Œä¸“ä¸ºé“¶è¡Œå®¢æœé¢†åŸŸçš„è¿è§„æ£€æµ‹æ¨¡å‹è®­ç»ƒè€Œè®¾è®¡ã€‚é€šè¿‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åŠ¨æ€Promptç”ŸæˆæŠ€æœ¯ï¼Œç»“åˆå¤šç»´åº¦å˜é‡ç»„åˆå’Œä¸¥æ ¼çš„è´¨é‡æ§åˆ¶æœºåˆ¶ï¼Œè‡ªåŠ¨åŒ–ç”Ÿæˆé«˜è´¨é‡ã€å¤šæ ·åŒ–çš„å®¢æœå¯¹è¯è®­ç»ƒæ ·æœ¬ã€‚

### æ ¸å¿ƒä»·å€¼

âœ… **è§£å†³æ•°æ®åŒ®ä¹**ï¼šè‡ªåŠ¨åŒ–ç”Ÿæˆå¤§è§„æ¨¡è®­ç»ƒæ•°æ®ï¼Œé™ä½äººå·¥æ ‡æ³¨æˆæœ¬  
âœ… **ä¿è¯æ•°æ®è´¨é‡**ï¼šå¤šå±‚æ¬¡è´¨é‡æ£€æŸ¥å’Œè¯­ä¹‰å»é‡æœºåˆ¶  
âœ… **è¦†ç›–å…¨é¢åœºæ™¯**ï¼šæ”¯æŒ8å¤§ä¸šåŠ¡åœºæ™¯ã€10+è¿è§„ç±»å‹çš„ç»„åˆ  
âœ… **é«˜åº¦å¯é…ç½®**ï¼šé€šè¿‡YAMLé…ç½®çµæ´»è°ƒæ•´ç”Ÿæˆç­–ç•¥  
âœ… **é«˜æ€§èƒ½å¤„ç†**ï¼šå¼‚æ­¥å¹¶å‘æ¶æ„ï¼Œæ”¯æŒå¤§è§„æ¨¡æ‰¹é‡ç”Ÿæˆ

### åº”ç”¨åœºæ™¯

- ğŸ¦ é“¶è¡Œå®¢æœè¿è§„æ£€æµ‹æ¨¡å‹è®­ç»ƒ
- ğŸ“ ç”µè¯è¥é”€åˆè§„æ€§ç›‘æ§
- ğŸ’¬ åœ¨çº¿å®¢æœè´¨é‡è¯„ä¼°
- ğŸ“ é‡‘èä»ä¸šäººå‘˜åŸ¹è®­
- ğŸ” ç›‘ç®¡åˆè§„æ€§å®¡æŸ¥

---

## â­ æ ¸å¿ƒç‰¹æ€§

### 1. å¤šç»´åº¦å˜é‡ç®¡ç†

ç³»ç»Ÿæ”¯æŒä»¥ä¸‹ç»´åº¦çš„çµæ´»ç»„åˆï¼š

| ç»´åº¦ | è¯´æ˜ | é…ç½®æ–‡ä»¶ |
|-----|------|---------|
| **ä¸šåŠ¡åœºæ™¯** | å­˜æ¬¾ã€è´·æ¬¾ã€ç†è´¢ã€ä¿¡ç”¨å¡ç­‰8å¤§ç±» | `core_business.yaml` |
| **è¿è§„ç±»å‹** | ä¿æœ¬æ‰¿è¯ºã€å¤¸å¤§å®£ä¼ ã€é¥¥é¥¿è¥é”€ç­‰10+ç±» | `violation_types.yaml` |
| **ç”¨æˆ·ç”»åƒ** | å¹´é¾„ã€èŒä¸šã€é£é™©åå¥½ã€æ€§æ ¼ç‰¹å¾ | `user_profiles.yaml` |
| **å¯¹è¯é£æ ¼** | ä¸“ä¸šå‹ã€äº²åˆ‡å‹ã€æ€¥åŠŸè¿‘åˆ©å‹ç­‰ | `dialogue_styles.yaml` |
| **å¯¹è¯å¤æ‚åº¦** | ç®€å•(1-3è½®)ã€ä¸­ç­‰(4-8è½®)ã€å¤æ‚(9-12è½®) | åŠ¨æ€ç”Ÿæˆ |

### 2. æ™ºèƒ½Promptç”Ÿæˆ

- **è½®ç›˜èµŒç®—æ³•**ï¼šåŸºäºæƒé‡çš„æ™ºèƒ½å˜é‡é€‰æ‹©
- **Few-Shotå¢å¼º**ï¼šåŠ¨æ€æ’å…¥åŒç±»ç¤ºä¾‹æé«˜ç”Ÿæˆè´¨é‡
- **å¤šè¿è§„ç»„åˆ**ï¼šæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªè¿è§„ç±»å‹ç»„åˆï¼ˆ70%å•ä¸ªï¼Œ30%å¤šä¸ªï¼‰
- **æ¯”ä¾‹æ§åˆ¶**ï¼šå¯é…ç½®è¿è§„/åˆè§„æ ·æœ¬æ¯”ä¾‹ï¼ˆé»˜è®¤30%è¿è§„ï¼‰


### 3. ä¸¥æ ¼è´¨é‡æ§åˆ¶

- âœ“ **å¿…éœ€å­—æ®µæ£€æŸ¥**ï¼šç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå®Œæ•´
- âœ“ **åˆè§„æ€§æ ‡æ³¨éªŒè¯**ï¼šéªŒè¯è¿è§„æ ‡æ³¨ä¸å¯¹è¯å†…å®¹ä¸€è‡´æ€§
- âœ“ **å¯¹è¯è´¨é‡æ£€æŸ¥**ï¼šè½®æ¬¡èŒƒå›´(3-10)ã€é•¿åº¦èŒƒå›´(50-2000å­—ç¬¦)
- âœ“ **è¯­ä¹‰å»é‡**ï¼šä½¿ç”¨Sentence-Transformersè®¡ç®—ç›¸ä¼¼åº¦(é˜ˆå€¼0.85)
- âœ“ **æ•æ„Ÿä¿¡æ¯è¿‡æ»¤**ï¼šè‡ªåŠ¨æ£€æµ‹å’Œè¿‡æ»¤èº«ä»½è¯ã€é“¶è¡Œå¡å·ç­‰

### 4. é«˜æ€§èƒ½æ¶æ„

- âš¡ **å¼‚æ­¥å¹¶å‘**ï¼šåŸºäºasyncioçš„å…¨å¼‚æ­¥å¤„ç†
- ğŸ”„ **æ‰¹é‡ç”Ÿæˆ**ï¼šæ”¯æŒæ‰¹é‡å¹¶å‘ï¼Œå¯é…ç½®å¹¶å‘æ•°
- ğŸ’¾ **æµå¼è¾“å‡º**ï¼šå®æ—¶å†™å…¥JSONLæ–‡ä»¶ï¼Œé™ä½å†…å­˜å ç”¨
- ğŸ” **æ™ºèƒ½é‡è¯•**ï¼šæŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼Œåº”å¯¹ç½‘ç»œæŠ–åŠ¨
- ğŸ“Š **å®æ—¶ç»Ÿè®¡**ï¼šç”Ÿæˆè¿›åº¦ã€è´¨é‡æŒ‡æ ‡å®æ—¶è¿½è¸ª

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "é…ç½®å±‚ Configuration Layer"
        A1[YAMLé…ç½®æ–‡ä»¶]
        A2[ç¯å¢ƒå˜é‡]
        A3[ConfigManager]
    end
    
    subgraph "æ ¸å¿ƒå¤„ç†å±‚ Core Processing Layer"
        B1[GenerationPipeline<br/>ä¸»æµæ°´çº¿]
        B2[VariableManager<br/>å˜é‡ç®¡ç†]
        B3[PromptGenerator<br/>Promptç”Ÿæˆ]
        B4[QualityController<br/>è´¨é‡æ§åˆ¶]
    end
    
    subgraph "åŸºç¡€è®¾æ–½å±‚ Infrastructure Layer"
        C1[LLMClient<br/>LLMå®¢æˆ·ç«¯]
        C2[LLMProvideræ¥å£]
        C3[OpenAI Provider]
        C4[DashScope Provider]
    end
    
    subgraph "æ•°æ®è¾“å‡ºå±‚ Data Output Layer"
        D1[DataSinkæ¥å£]
        D2[JsonlFileSink]
        D3[JSONLæ–‡ä»¶]
    end
    
    A1 --> A3
    A2 --> A3
    A3 --> B1
    
    B1 --> B2
    B1 --> B3
    B1 --> B4
    B1 --> C1
    B1 --> D1
    
    B2 -.å˜é‡ç»„åˆ.-> B3
    
    C1 --> C2
    C2 --> C3
    C2 --> C4
    
    D1 --> D2
    D2 --> D3
    
    style B1 fill:#ff9,stroke:#333,stroke-width:3px
    style C1 fill:#9cf,stroke:#333,stroke-width:2px
    style D2 fill:#9f9,stroke:#333,stroke-width:2px
```

### æ•°æ®æµå›¾

```mermaid
graph LR
    A[é…ç½®åŠ è½½] -->|YAMLè§£æ| B[å˜é‡æ± åˆå§‹åŒ–]
    B -->|åŠ æƒéšæœºé‡‡æ ·| C[é€‰æ‹©å˜é‡ç»„åˆ]
    C -->|æ¨¡æ¿å¡«å……| D[ç”ŸæˆMeta Prompt]
    D -->|APIè°ƒç”¨| E[LLMç”Ÿæˆå“åº”]
    E -->|JSONè§£æ| F[ç»“æ„åŒ–æ ·æœ¬]
    F -->|è´¨é‡æ£€æŸ¥| G{è´¨é‡åˆæ ¼?}
    G -->|Yes| H[è¯­ä¹‰å»é‡]
    G -->|No| I[ä¸¢å¼ƒ/é‡è¯•]
    H -->|é€šè¿‡| J[å†™å…¥JSONL]
    H -->|é‡å¤| I
    J --> K[æ›´æ–°ç»Ÿè®¡]
    
    style D fill:#ffd,stroke:#333,stroke-width:2px
    style G fill:#faa,stroke:#333,stroke-width:2px
    style J fill:#afa,stroke:#333,stroke-width:2px
```


### æ ¸å¿ƒæµç¨‹å›¾

```mermaid
flowchart TD
    Start([å¼€å§‹]) --> Init[åˆå§‹åŒ–ç»„ä»¶]
    Init --> CheckEnv{ç¯å¢ƒæ£€æŸ¥}
    CheckEnv -->|å¤±è´¥| Exit([é€€å‡º])
    CheckEnv -->|é€šè¿‡| LoadConfig[åŠ è½½é…ç½®]
    
    LoadConfig --> LoopStart{è¾¾åˆ°ç›®æ ‡æ•°é‡?}
    LoopStart -->|æ˜¯| Finish([å®Œæˆ])
    LoopStart -->|å¦| BatchLoop[å¼€å§‹æ–°æ‰¹æ¬¡]
    
    subgraph Batch ["å¼‚æ­¥æ‰¹é‡å¤„ç†"]
        BatchLoop --> SelectVars[é€‰æ‹©å˜é‡ç»„åˆ]
        SelectVars --> GenPrompt[ç”ŸæˆMeta Prompt]
        GenPrompt --> CallLLM[è°ƒç”¨LLMç”Ÿæˆ]
        CallLLM --> Parse[è§£æJSONå“åº”]
        Parse --> QualityCheck{è´¨é‡æ£€æŸ¥}
        QualityCheck -->|é€šè¿‡| SimilarityCheck{ç›¸ä¼¼åº¦æ£€æŸ¥}
        QualityCheck -->|å¤±è´¥| RecordFail[è®°å½•å¤±è´¥]
        SimilarityCheck -->|é€šè¿‡| AddBatch[åŠ å…¥æ‰¹æ¬¡åˆ—è¡¨]
        SimilarityCheck -->|é‡å¤| RecordDup[è®°å½•é‡å¤]
    end
    
    AddBatch --> WriteFile[æ‰¹é‡å†™å…¥æ–‡ä»¶]
    RecordFail --> LoopStart
    RecordDup --> LoopStart
    WriteFile --> UpdateStats[æ›´æ–°ç»Ÿè®¡ä¿¡æ¯]
    UpdateStats --> Progress{éœ€è¦æŠ¥å‘Šè¿›åº¦?}
    Progress -->|æ˜¯| PrintProgress[æ‰“å°è¿›åº¦]
    Progress -->|å¦| LoopStart
    PrintProgress --> LoopStart
    
    Finish --> FinalStats[æ‰“å°æœ€ç»ˆç»Ÿè®¡]
    FinalStats --> End([ç»“æŸ])
    
    style Batch fill:#e6f3ff,stroke:#333,stroke-width:2px
    style QualityCheck fill:#ffe6e6,stroke:#333,stroke-width:2px
    style WriteFile fill:#e6ffe6,stroke:#333,stroke-width:2px
```

---

## ğŸ”§ æŠ€æœ¯æ ˆ

### æ ¸å¿ƒæŠ€æœ¯

| æŠ€æœ¯ | ç‰ˆæœ¬ | ç”¨é€” |
|-----|------|------|
| **Python** | 3.8+ | ä¸»è¦å¼€å‘è¯­è¨€ |
| **asyncio** | æ ‡å‡†åº“ | å¼‚æ­¥å¹¶å‘å¤„ç† |
| **aiohttp** | 3.8+ | å¼‚æ­¥HTTPå®¢æˆ·ç«¯ |
| **aiofiles** | 23.0+ | å¼‚æ­¥æ–‡ä»¶IO |
| **PyYAML** | 6.0+ | é…ç½®æ–‡ä»¶è§£æ |

### LLMé›†æˆ

| åº“ | ç‰ˆæœ¬ | ç”¨é€” |
|-----|------|------|
| **openai** | 1.0+ | OpenAI APIå®¢æˆ·ç«¯ |
| **requests** | 2.28+ | åŒæ­¥HTTPè¯·æ±‚ |

### è´¨é‡æ§åˆ¶

| åº“ | ç‰ˆæœ¬ | ç”¨é€” |
|-----|------|------|
| **sentence-transformers** | 2.2+ | è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®— |
| **torch** | 1.13+ | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| **numpy** | 1.24+ | æ•°å€¼è®¡ç®— |

### å·¥å…·åº“

| åº“ | ç‰ˆæœ¬ | ç”¨é€” |
|-----|------|------|
| **python-dotenv** | 1.0+ | ç¯å¢ƒå˜é‡ç®¡ç† |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd sample_generator

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```


### 2. é…ç½®ç¯å¢ƒå˜é‡

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```ini
# OpenAI APIé…ç½®
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_BASE_URL=https://api.openai.com/v1/chat/completions

# æˆ–ä½¿ç”¨é˜¿é‡Œäº‘DashScope
# OPENAI_API_KEY=sk-xxxxxxxxxxxx
# OPENAI_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
```

### 3. è¿è¡Œç”Ÿæˆ

```bash
# ç”Ÿæˆ10ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤ï¼‰
python main.py

# ç”Ÿæˆ100ä¸ªæ ·æœ¬
python main.py --samples 100

# æŒ‡å®šè¾“å‡ºè·¯å¾„
python main.py --samples 100 --output data/train.jsonl

# è°ƒè¯•æ¨¡å¼
python main.py --samples 10 --log-level DEBUG
```

### 4. æŸ¥çœ‹ç»“æœ

ç”Ÿæˆçš„JSONLæ–‡ä»¶ä½äº `outputs/` ç›®å½•ï¼Œæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼š

```json
{
  "id": 1,
  "sample": {
    "instruction": "åˆ†æä»¥ä¸‹é“¶è¡Œå®¢æœå¯¹è¯æ˜¯å¦è¿è§„ï¼Œè‹¥è¿è§„è¯·æŒ‡å‡ºåŸæ–‡ã€ç±»åˆ«åŠç›‘ç®¡ä¾æ®",
    "input": "<å®¢æœ> æ‚¨å¥½ï¼Œæ¬¢è¿è‡´ç”µXXé“¶è¡Œ...\n<å®¢æˆ·> ...",
    "output": {
      "is_compliant": false,
      "violated_text": "æœ¬é‡‘ç»å¯¹å®‰å…¨ï¼Œæ²¡æœ‰ä»»ä½•é£é™©",
      "violation_type": "æ”¶ç›Šæ‰¿è¯ºä¸ä¿æœ¬è¯¯å¯¼",
      "judgment_basis": "è¿åã€Šå…³äºè¿›ä¸€æ­¥è§„èŒƒé‡‘èè¥é”€å®£ä¼ è¡Œä¸ºçš„é€šçŸ¥ã€‹...",
      "confidence": 0.98
    },
    "metadata": {
      "business_scene": "ç†è´¢ä¸šåŠ¡-ç†è´¢è´­ä¹°",
      "violation_type": "æ”¶ç›Šæ‰¿è¯ºä¸ä¿æœ¬è¯¯å¯¼",
      "user_profile": "ä¿å®ˆå‹è€å¹´å®¢æˆ·",
      "dialogue_style": "è¿‡åº¦æ‰¿è¯º"
    }
  }
}
```

---

## ğŸ“š è¯¦ç»†æ–‡æ¡£

### æ¨¡å—è®¾è®¡

#### 1. ConfigManager - é…ç½®ç®¡ç†å™¨

**èŒè´£**ï¼šåŠ è½½å’Œç®¡ç†æ‰€æœ‰YAMLé…ç½®æ–‡ä»¶

**æ ¸å¿ƒæ–¹æ³•**ï¼š
- `load_all_configs()`: åŠ è½½æ‰€æœ‰å¿…éœ€é…ç½®
- `get_llm_config()`: è·å–LLMé…ç½®
- `get_generation_config()`: è·å–ç”Ÿæˆé…ç½®
- `get_quality_config()`: è·å–è´¨é‡é…ç½®

**é…ç½®æ–‡ä»¶**ï¼š
```
config/
â”œâ”€â”€ main_config.yaml       # ä¸»é…ç½®ï¼ˆLLMã€ç”Ÿæˆå‚æ•°ï¼‰
â”œâ”€â”€ core_business.yaml     # ä¸šåŠ¡åœºæ™¯å®šä¹‰
â”œâ”€â”€ violation_types.yaml   # è¿è§„ç±»å‹åŠç¤ºä¾‹
â”œâ”€â”€ user_profiles.yaml     # ç”¨æˆ·ç”»åƒ
â”œâ”€â”€ dialogue_styles.yaml   # å¯¹è¯é£æ ¼
â””â”€â”€ quality_rules.yaml     # è´¨é‡è§„åˆ™
```

#### 2. VariableManager - å˜é‡ç®¡ç†å™¨

**èŒè´£**ï¼šåŸºäºæƒé‡çš„å˜é‡é€‰æ‹©å’Œç»„åˆ

**æ ¸å¿ƒç®—æ³•**ï¼šè½®ç›˜èµŒç®—æ³•ï¼ˆRoulette Wheel Selectionï¼‰

```python
def weighted_choice(items):
    total_weight = sum(item['weight'] for item in items)
    random_value = random.uniform(0, total_weight)
    cumulative_weight = 0
    for item in items:
        cumulative_weight += item['weight']
        if cumulative_weight >= random_value:
            return item
```

**æ ¸å¿ƒæ–¹æ³•**ï¼š
- `select_variables()`: é€‰æ‹©å®Œæ•´å˜é‡ç»„åˆ
- `weighted_choice()`: åŠ æƒéšæœºé€‰æ‹©
- `get_stats()`: è·å–ç”Ÿæˆç»Ÿè®¡


#### 3. PromptGenerator - Promptç”Ÿæˆå™¨

**èŒè´£**ï¼šåŠ¨æ€ç”ŸæˆMeta Prompt

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
- å˜é‡ç»„åˆå¡«å……
- Few-Shotç¤ºä¾‹æ’å…¥
- åˆè§„æ€§æ£€æŸ¥ï¼ˆæ•æ„Ÿä¿¡æ¯è¿‡æ»¤ï¼‰
- éšæœºåç¼€æ·»åŠ ï¼ˆé˜²ç¼“å­˜ï¼‰

**Promptæ¨¡æ¿ç»“æ„**ï¼š
```
ä½ æ˜¯ä¸€ä½é“¶è¡Œå®¢æœæ•°æ®ç”Ÿæˆä¸“å®¶...

### çº¦æŸæ¡ä»¶
1. ä¸šåŠ¡åœºæ™¯ï¼š{business_scene} - {sub_scene}
2. å¯¹è¯ç±»å‹ï¼š{è¿è§„/åˆè§„}
3. è¿è§„ç±»å‹ï¼š{violation_type}
4. ç”¨æˆ·ç”»åƒï¼š{user_profile}
5. å¯¹è¯é£æ ¼ï¼š{dialogue_style}
6. å¤æ‚ç¨‹åº¦ï¼š{complexity}

### ç¤ºä¾‹å¼•å¯¼
{violation_example}

### è¾“å‡ºè¦æ±‚
è¯·ç”Ÿæˆå•ä¸ªå®Œæ•´æ ·æœ¬ï¼Œç›´æ¥è¾“å‡ºJSONæ ¼å¼...
```

#### 4. LLMClient - LLMå®¢æˆ·ç«¯

**èŒè´£**ï¼šå°è£…LLM APIè°ƒç”¨

**æ”¯æŒçš„Provider**ï¼š
- OpenAI (GPT-3.5/GPT-4)
- DashScope (é€šä¹‰åƒé—®)
- å¯æ‰©å±•å…¶ä»–Provider

**æ ¸å¿ƒæ–¹æ³•**ï¼š
- `generate_text_async()`: å¼‚æ­¥ç”Ÿæˆæ–‡æœ¬
- `generate_texts_async()`: æ‰¹é‡å¼‚æ­¥ç”Ÿæˆ

**é”™è¯¯å¤„ç†**ï¼š
- æŒ‡æ•°é€€é¿é‡è¯•ï¼ˆ2^nç§’ï¼‰
- é€Ÿç‡é™åˆ¶å¤„ç†
- è¶…æ—¶å¤„ç†ï¼ˆé»˜è®¤30ç§’ï¼‰

#### 5. QualityController - è´¨é‡æ§åˆ¶å™¨

**èŒè´£**ï¼šæ ·æœ¬è´¨é‡æ£€æŸ¥å’Œå»é‡

**æ£€æŸ¥é¡¹**ï¼š

| æ£€æŸ¥é¡¹ | è¯´æ˜ | æ‰£åˆ† |
|-------|------|------|
| å¿…éœ€å­—æ®µ | æ£€æŸ¥æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨ | -10åˆ†/å­—æ®µ |
| åˆè§„æ€§æ ‡æ³¨ | éªŒè¯è¿è§„æ ‡æ³¨å‡†ç¡®æ€§ | -20åˆ† |
| å¯¹è¯è´¨é‡ | è½®æ¬¡(3-10)ã€é•¿åº¦(50-2000) | -20åˆ† |
| è¯­è¨€è´¨é‡ | å†…å®¹é•¿åº¦åˆç†æ€§ | -5åˆ†/é—®é¢˜ |

**è¯­ä¹‰å»é‡**ï¼š
```python
# ä½¿ç”¨Sentence-Transformersè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
text_embedding = model.encode(text, convert_to_tensor=True)
similarities = util.cos_sim(text_embedding, existing_embeddings)
max_similarity = similarities.max().item()
is_duplicate = max_similarity >= 0.85  # é˜ˆå€¼
```

#### 6. GenerationPipeline - ä¸»æµæ°´çº¿

**èŒè´£**ï¼šåè°ƒæ‰€æœ‰ç»„ä»¶å®Œæˆç«¯åˆ°ç«¯ç”Ÿæˆ

**æ ¸å¿ƒæµç¨‹**ï¼š
1. åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
2. æ‰¹é‡å¼‚æ­¥ç”Ÿæˆæ ·æœ¬
3. è´¨é‡æ£€æŸ¥å’Œå»é‡
4. æµå¼å†™å…¥æ–‡ä»¶
5. å®æ—¶ç»Ÿè®¡å’Œè¿›åº¦æŠ¥å‘Š

**ç»Ÿè®¡æŒ‡æ ‡**ï¼š
- `generated_count`: å·²ç”Ÿæˆæ ·æœ¬æ•°
- `saved_count`: å·²ä¿å­˜æ ·æœ¬æ•°
- `filtered_count`: å·²è¿‡æ»¤æ ·æœ¬æ•°
- `quality_rejected_count`: è´¨é‡ä¸åˆæ ¼æ•°
- `duplicate_rejected_count`: é‡å¤è¿‡æ»¤æ•°
- `parse_error_count`: è§£æé”™è¯¯æ•°


### UMLç»„ä»¶å›¾

```mermaid
classDiagram
    class GenerationPipeline {
        -config_manager: ConfigManager
        -variable_manager: VariableManager
        -prompt_generator: PromptGenerator
        -llm_client: LLMClient
        -quality_controller: QualityController
        +generate_samples_async(num_samples, output_path)
        -_generate_batch_async(batch_size)
        -_generate_single_sample_async()
        -_parse_response(response, vars)
    }
    
    class ConfigManager {
        -config_dir: Path
        -configs: Dict
        +load_all_configs()
        +get_llm_config()
        +get_generation_config()
        +get_quality_config()
    }
    
    class VariableManager {
        -configs: Dict
        -generation_stats: Dict
        +select_variables()
        +weighted_choice(items)
        +get_stats()
        -_select_business_context()
        -_select_violation_types()
    }
    
    class PromptGenerator {
        -configs: Dict
        +generate_meta_prompt()
        +check_compliance(prompt)
        -_build_meta_prompt(vars, example, is_compliant)
        -_get_violation_example(vars)
    }
    
    class LLMClient {
        -api_key: str
        -model: str
        -base_url: str
        +generate_text_async(prompt)
        +generate_texts_async(prompts)
        -_prepare_headers()
        -_prepare_payload(prompt, config)
    }
    
    class QualityController {
        -configs: Dict
        -sim_model: SentenceTransformer
        -generated_embeddings: deque
        +check_sample(sample)
        +check_similarity(text)
        +add_sample(sample)
        -_check_field_exists(sample, field)
        -_check_compliance_accuracy(sample, rules)
    }
    
    class LLMProvider {
        <<interface>>
        +prepare_request(request)
        +parse_response(response)
        +get_rate_limit_policy()
    }
    
    class OpenAIChatProvider {
        +prepare_request(request)
        +parse_response(response)
    }
    
    class DashScopeProvider {
        +prepare_request(request)
        +parse_response(response)
    }
    
    class DataSink {
        <<interface>>
        +write_batch(samples)
        +flush()
        +close()
    }
    
    class JsonlFileSink {
        -output_path: Path
        -file: File
        +write_batch(samples)
        +flush()
        +close()
    }
    
    GenerationPipeline --> ConfigManager
    GenerationPipeline --> VariableManager
    GenerationPipeline --> PromptGenerator
    GenerationPipeline --> LLMClient
    GenerationPipeline --> QualityController
    GenerationPipeline --> DataSink
    
    LLMClient --> LLMProvider
    LLMProvider <|-- OpenAIChatProvider
    LLMProvider <|-- DashScopeProvider
    
    DataSink <|-- JsonlFileSink
```


### æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant Main as main.py
    participant Pipeline as GenerationPipeline
    participant VarMgr as VariableManager
    participant PromptGen as PromptGenerator
    participant LLM as LLMClient
    participant QC as QualityController
    participant Sink as JsonlFileSink
    
    Main->>Pipeline: generate_samples_async(100)
    activate Pipeline
    
    loop æ‰¹æ¬¡å¾ªç¯
        Pipeline->>Pipeline: _generate_batch_async(50)
        
        par å¹¶å‘ç”Ÿæˆ
            Pipeline->>VarMgr: select_variables()
            activate VarMgr
            VarMgr-->>Pipeline: selected_vars
            deactivate VarMgr
            
            Pipeline->>PromptGen: generate_meta_prompt()
            activate PromptGen
            PromptGen->>VarMgr: select_variables()
            VarMgr-->>PromptGen: vars
            PromptGen-->>Pipeline: meta_prompt, vars
            deactivate PromptGen
            
            Pipeline->>LLM: generate_text_async(prompt)
            activate LLM
            LLM->>LLM: é‡è¯•æœºåˆ¶
            LLM-->>Pipeline: response_text
            deactivate LLM
            
            Pipeline->>Pipeline: _parse_response()
            
            Pipeline->>QC: check_sample(sample)
            activate QC
            QC-->>Pipeline: quality_result
            deactivate QC
            
            alt è´¨é‡é€šè¿‡
                Pipeline->>QC: check_similarity(text)
                activate QC
                QC-->>Pipeline: is_unique
                deactivate QC
                
                alt ä¸é‡å¤
                    Pipeline->>QC: add_sample(sample)
                    Pipeline->>Pipeline: åŠ å…¥æ‰¹æ¬¡åˆ—è¡¨
                else é‡å¤
                    Pipeline->>Pipeline: è®°å½•é‡å¤
                end
            else è´¨é‡ä¸åˆæ ¼
                Pipeline->>Pipeline: è®°å½•å¤±è´¥
            end
        end
        
        Pipeline->>Sink: write_batch(samples)
        activate Sink
        Sink-->>Pipeline: written_count
        deactivate Sink
        
        Pipeline->>Pipeline: æ›´æ–°ç»Ÿè®¡
        Pipeline->>Pipeline: æ‰“å°è¿›åº¦
    end
    
    Pipeline-->>Main: å®Œæˆ
    deactivate Pipeline
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### ä¸»é…ç½®æ–‡ä»¶ (main_config.yaml)

```yaml
generation:
  batch_size: 50              # æ‰¹æ¬¡å¤§å°
  target_count: 20000         # ç›®æ ‡æ ·æœ¬æ•°
  max_retries: 3              # æœ€å¤§é‡è¯•æ¬¡æ•°
  temperature: 0.5            # LLMæ¸©åº¦å‚æ•°
  concurrency_limit: 2        # å¹¶å‘é™åˆ¶

llm:
  model: "gpt-4.1-mini"       # æ¨¡å‹åç§°
  api_key: "${OPENAI_API_KEY}" # APIå¯†é’¥ï¼ˆç¯å¢ƒå˜é‡ï¼‰
  base_url: "${OPENAI_BASE_URL}" # APIåœ°å€
  max_tokens: 2000            # æœ€å¤§tokenæ•°
  timeout: 30                 # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

output:
  file_pattern: "generated_samples_{timestamp}.jsonl"
  save_interval: 50           # ä¿å­˜é—´éš”

quality:
  min_score: 70               # æœ€ä½è´¨é‡åˆ†æ•°
  enable_deduplication: true  # å¯ç”¨å»é‡
  similarity_threshold: 0.8   # ç›¸ä¼¼åº¦é˜ˆå€¼
  violation_ratio: 0.3        # è¿è§„æ ·æœ¬æ¯”ä¾‹
```


### ä¸šåŠ¡åœºæ™¯é…ç½® (core_business.yaml)

```yaml
business_scenes:
  å­˜æ¬¾ä¸šåŠ¡:
    weight: 0.2              # æƒé‡
    sub_scenes:
      ä¸ªäººå­˜æ¬¾:
        variables: [æ´»æœŸå­˜æ¬¾, å®šæœŸå­˜æ¬¾, å¤§é¢å­˜å•, ...]
        weight: 0.5
      å¯¹å…¬å­˜æ¬¾:
        variables: [å•ä½æ´»æœŸå­˜æ¬¾, å•ä½å®šæœŸå­˜æ¬¾, ...]
        weight: 0.5
  
  é“¶è¡Œå¡ä¸šåŠ¡:
    weight: 0.15
    sub_scenes:
      è´¦æˆ·ç®¡ç†:
        variables: [å¼€å¡, æŒ‚å¤±, è¡¥å¡, é”€æˆ·, ...]
        weight: 0.4
      # ... æ›´å¤šå­åœºæ™¯
```

### è¿è§„ç±»å‹é…ç½® (violation_types.yaml)

```yaml
violation_types:
  æ”¶ç›Šæ‰¿è¯ºä¸ä¿æœ¬è¯¯å¯¼ç±»:
    weight: 0.12
    examples:
      - "ä¿æœ¬ä¿æ¯ï¼Œæ‚¨æ”¾å¿ƒä¹°"
      - "è¿™æ¬¾äº§å“é›¶é£é™©ï¼Œæœ¬é‡‘ç»å¯¹å®‰å…¨"
      - "å’Œå­˜æ¬¾ä¸€æ ·å®‰å…¨ï¼Œåˆ°æœŸè¿˜æœ¬ä»˜æ¯"
  
  ç»å¯¹åŒ–ä¸å¤¸å¤§å®£ä¼ ç±»:
    weight: 0.12
    examples:
      - "æˆ‘ä»¬çš„ä¿é™©æ˜¯è¡Œä¸šæœ€ä½³ï¼Œå…¨ç½‘é”€é‡å† å†›"
      - "è¿™æ¬¾äº§å“100%ç†èµ”ï¼Œä»€ä¹ˆéƒ½èƒ½ä¿"
  
  åˆè§„è¯æœ¯:
    weight: 0.18
    examples:
      - "è¿™æ¬¾äº§å“æœ‰15å¤©çŠ¹è±«æœŸï¼ŒçŠ¹è±«æœŸå†…é€€ä¿æ²¡æœ‰æŸå¤±"
      - "éœ€è¦æé†’æ‚¨ï¼Œä¿é™©æ”¶ç›Šæ˜¯ä¸ç¡®å®šçš„"
```

---

## ğŸ“– ä½¿ç”¨æŒ‡å—

### å‘½ä»¤è¡Œå‚æ•°

```bash
python main.py [OPTIONS]
```

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `--samples` | int | 10 | ç”Ÿæˆæ ·æœ¬æ•°é‡ |
| `--output` | str | è‡ªåŠ¨ç”Ÿæˆ | è¾“å‡ºæ–‡ä»¶è·¯å¾„ |
| `--config-dir` | str | config | é…ç½®æ–‡ä»¶ç›®å½• |
| `--mode` | str | async | è¿è¡Œæ¨¡å¼(sync/async) |
| `--log-level` | str | INFO | æ—¥å¿—çº§åˆ«(DEBUG/INFO/WARNING/ERROR) |

### ä½¿ç”¨ç¤ºä¾‹

#### 1. åŸºç¡€ä½¿ç”¨

```bash
# ç”Ÿæˆ10ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤ï¼‰
python main.py

# ç”Ÿæˆ1000ä¸ªæ ·æœ¬
python main.py --samples 1000
```

#### 2. æŒ‡å®šè¾“å‡ºè·¯å¾„

```bash
# æŒ‡å®šè¾“å‡ºæ–‡ä»¶
python main.py --samples 500 --output data/train_v1.jsonl

# è¾“å‡ºåˆ°ç‰¹å®šç›®å½•
python main.py --samples 500 --output outputs/$(date +%Y%m%d)/samples.jsonl
```

#### 3. è°ƒè¯•æ¨¡å¼

```bash
# å¼€å¯DEBUGæ—¥å¿—
python main.py --samples 10 --log-level DEBUG

# æŸ¥çœ‹è¯¦ç»†ç”Ÿæˆè¿‡ç¨‹
python main.py --samples 5 --log-level DEBUG 2>&1 | tee debug.log
```

#### 4. ä½¿ç”¨ä¸åŒé…ç½®

```bash
# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®ç›®å½•
python main.py --samples 100 --config-dir my_config
```


### è¾“å‡ºæ ¼å¼è¯´æ˜

ç”Ÿæˆçš„JSONLæ–‡ä»¶æ¯è¡ŒåŒ…å«ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ ·æœ¬ï¼š

```json
{
  "id": 1,                    // æ ·æœ¬ID
  "sample": {
    "instruction": "...",     // ä»»åŠ¡æŒ‡ä»¤
    "input": "...",           // å¯¹è¯å†…å®¹
    "output": {               // æ ‡æ³¨ç»“æœ
      "is_compliant": false,  // æ˜¯å¦åˆè§„
      "violated_text": "...", // è¿è§„åŸæ–‡
      "violation_type": "...",// è¿è§„ç±»å‹
      "judgment_basis": "...",// åˆ¤å®šä¾æ®
      "confidence": 0.98      // ç½®ä¿¡åº¦
    },
    "metadata": {             // å…ƒæ•°æ®
      "business_scene": "...",
      "violation_type": "...",
      "user_profile": "...",
      "dialogue_style": "..."
    }
  }
}
```

### æ—¥å¿—è¯´æ˜

ç³»ç»Ÿä¼šåœ¨ `logs/` ç›®å½•ç”Ÿæˆæ—¥å¿—æ–‡ä»¶ï¼š

```
logs/
â””â”€â”€ sample_generator.log    # ä¸»æ—¥å¿—æ–‡ä»¶ï¼ˆè‡ªåŠ¨è½®è½¬ï¼Œæœ€å¤šä¿ç•™5ä¸ªï¼‰
```

æ—¥å¿—æ ¼å¼ï¼š
```
2025-12-11 10:30:45 - sample_generator - INFO - å¼€å§‹å¼‚æ­¥ç”Ÿæˆ 100 ä¸ªæ ·æœ¬
2025-12-11 10:30:46 - sample_generator - INFO - å¤„ç†æ‰¹æ¬¡ 1/2, å¤§å°: 50
2025-12-11 10:31:15 - sample_generator - INFO - æ‰¹æ¬¡ 1 å†™å…¥ 48 ä¸ªæ ·æœ¬
```

### ç»Ÿè®¡ä¿¡æ¯

ç”Ÿæˆå®Œæˆåä¼šè¾“å‡ºè¯¦ç»†ç»Ÿè®¡ï¼š

```
============================================================
ç”Ÿæˆå®Œæˆï¼æœ€ç»ˆç»Ÿè®¡:
============================================================
æ€»è€—æ—¶: 125.34ç§’
å·²ç”Ÿæˆæ ·æœ¬: 105
å·²ä¿å­˜æ ·æœ¬: 100
è¿‡æ»¤æ ·æœ¬: 5
  - è´¨é‡ä¸åˆæ ¼: 2
  - é‡å¤è¿‡æ»¤: 2
  - è§£æé”™è¯¯: 1
æˆåŠŸç‡: 95.2%
å¹³å‡é€Ÿåº¦: 0.80 æ ·æœ¬/ç§’
============================================================
```

---

## ğŸ”Œ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„LLM Provider

1. åˆ›å»ºProviderç±»ï¼š

```python
# providers/custom_provider.py
from .base import LLMProvider, LLMRequest, LLMResponse, ProviderConfig

class CustomProvider(LLMProvider):
    def prepare_request(self, request: LLMRequest):
        url = self.config.base_url
        headers = {"Authorization": f"Bearer {self.config.api_key}"}
        payload = {
            "model": self.config.model,
            "prompt": request.prompt,
            # ... è‡ªå®šä¹‰å­—æ®µ
        }
        return url, headers, payload
    
    def parse_response(self, raw_response):
        content = raw_response["result"]["text"]
        usage = raw_response.get("usage", {})
        return LLMResponse(
            content=content,
            usage=usage,
            model=self.config.model
        )
    
    def get_rate_limit_policy(self):
        return {
            "max_retries": 3,
            "backoff_base": 2,
            "max_delay": 60
        }
```

2. åœ¨ `pipeline.py` ä¸­æ³¨å†Œï¼š

```python
def _init_llm_client(self):
    base_url = llm_config.get("base_url", "")
    if "custom" in base_url.lower():
        provider = CustomProvider(provider_config)
    elif "dashscope" in base_url.lower():
        provider = DashScopeProvider(provider_config)
    else:
        provider = OpenAIChatProvider(provider_config)
```


### æ·»åŠ æ–°çš„ä¸šåŠ¡åœºæ™¯

ç›´æ¥ç¼–è¾‘ `config/core_business.yaml`ï¼š

```yaml
business_scenes:
  # æ·»åŠ æ–°åœºæ™¯
  ä¿é™©ä¸šåŠ¡:
    weight: 0.1
    sub_scenes:
      å¯¿é™©:
        variables: [å®šæœŸå¯¿é™©, ç»ˆèº«å¯¿é™©, ä¸¤å…¨ä¿é™©, å¹´é‡‘ä¿é™©]
        weight: 0.5
      å¥åº·é™©:
        variables: [é‡ç–¾é™©, åŒ»ç–—é™©, æŠ¤ç†é™©, å¤±èƒ½é™©]
        weight: 0.5
```

### æ·»åŠ æ–°çš„è¿è§„ç±»å‹

ç¼–è¾‘ `config/violation_types.yaml`ï¼š

```yaml
violation_types:
  # æ·»åŠ æ–°è¿è§„ç±»å‹
  è™šå‡å®£ä¼ ç±»:
    weight: 0.08
    examples:
      - "æˆ‘ä»¬æ˜¯å”¯ä¸€è·å¾—XXè®¤è¯çš„é“¶è¡Œ"
      - "æœ¬äº§å“å·²è·å¾—å›½å®¶çº§å¥–é¡¹"
      - "å¤®è¡Œæ¨èçš„ç†è´¢äº§å“"
```

### è‡ªå®šä¹‰è´¨é‡æ£€æŸ¥è§„åˆ™

ç¼–è¾‘ `config/quality_rules.yaml`ï¼š

```yaml
required_fields:
  - instruction
  - input
  - output.is_compliant
  - output.violated_text
  - output.violation_type

compliance_check:
  violation_must_have_content: true
  compliant_must_not_have_content: true

min_turns: 3
max_turns: 15
min_dialogue_length: 50
max_dialogue_length: 2000

language_quality:
  min_content_length: 3
  max_content_length: 200
```

### æ·»åŠ æ–°çš„DataSink

1. åˆ›å»ºSinkç±»ï¼š

```python
# sinks/database_sink.py
from .base import DataSink

class DatabaseSink(DataSink):
    def __init__(self, connection_string):
        self.conn = create_connection(connection_string)
    
    async def write_batch(self, samples):
        async with self.conn.cursor() as cursor:
            for sample in samples:
                await cursor.execute(
                    "INSERT INTO samples VALUES (%s, %s)",
                    (sample['id'], json.dumps(sample['sample']))
                )
        await self.conn.commit()
        return len(samples)
    
    async def flush(self):
        await self.conn.commit()
    
    async def close(self):
        await self.conn.close()
```

2. åœ¨ `pipeline.py` ä¸­ä½¿ç”¨ï¼š

```python
# æ›¿æ¢JsonlFileSink
from sinks import DatabaseSink
sink = DatabaseSink("postgresql://...")
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å¹¶å‘æ§åˆ¶

æ ¹æ®APIé€Ÿç‡é™åˆ¶è°ƒæ•´å¹¶å‘æ•°ï¼š

```yaml
# main_config.yaml
generation:
  concurrency_limit: 5  # å¢åŠ å¹¶å‘æ•°ï¼ˆæ³¨æ„APIé™åˆ¶ï¼‰
  batch_size: 100       # å¢åŠ æ‰¹æ¬¡å¤§å°
```

### 2. å†…å­˜ä¼˜åŒ–

å¯¹äºå¤§è§„æ¨¡ç”Ÿæˆï¼Œè°ƒæ•´ç¼“å­˜å¤§å°ï¼š

```python
# src/constants.py
class QualityConstants:
    MAX_EMBEDDINGS_CACHE = 500  # å‡å°‘ç¼“å­˜å¤§å°
    GC_THRESHOLD = 50           # æ›´é¢‘ç¹çš„åƒåœ¾å›æ”¶
```

### 3. å»é‡ä¼˜åŒ–

å¦‚æœä¸éœ€è¦ä¸¥æ ¼å»é‡ï¼Œå¯ä»¥ç¦ç”¨ï¼š

```yaml
# main_config.yaml
quality:
  enable_deduplication: false
```


### 4. æ¨¡å‹é€‰æ‹©

æ ¹æ®éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼š

| æ¨¡å‹ | é€Ÿåº¦ | è´¨é‡ | æˆæœ¬ | é€‚ç”¨åœºæ™¯ |
|-----|------|------|------|---------|
| gpt-3.5-turbo | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | å¤§è§„æ¨¡ç”Ÿæˆ |
| gpt-4.1-mini | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | å¹³è¡¡é€‰æ‹© |
| gpt-4 | â­â­â­ | â­â­â­â­â­ | â­â­ | é«˜è´¨é‡éœ€æ±‚ |
| qwen-turbo | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | å›½å†…éƒ¨ç½² |

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: APIè°ƒç”¨å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç¡®è®¤ `.env` æ–‡ä»¶ä¸­çš„APIå¯†é’¥æ­£ç¡®
2. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIåœ°å€
3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶äº†è§£è¯¦ç»†é”™è¯¯ä¿¡æ¯
4. å°è¯•é™ä½å¹¶å‘æ•° `concurrency_limit`

### Q2: ç”Ÿæˆçš„æ ·æœ¬è´¨é‡ä¸é«˜ï¼Ÿ

**A**: å¯ä»¥å°è¯•ï¼š
1. è°ƒæ•´æ¸©åº¦å‚æ•° `temperature`ï¼ˆé™ä½å¯æé«˜ä¸€è‡´æ€§ï¼‰
2. å¢åŠ Few-Shotç¤ºä¾‹æ•°é‡
3. ä¼˜åŒ–Promptæ¨¡æ¿
4. æé«˜è´¨é‡æ£€æŸ¥é˜ˆå€¼ `min_score`

### Q3: ç”Ÿæˆé€Ÿåº¦å¤ªæ…¢ï¼Ÿ

**A**: ä¼˜åŒ–å»ºè®®ï¼š
1. å¢åŠ å¹¶å‘æ•° `concurrency_limit`
2. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ï¼ˆå¦‚gpt-3.5-turboï¼‰
3. ç¦ç”¨å»é‡åŠŸèƒ½ï¼ˆå¦‚æœä¸éœ€è¦ï¼‰
4. å‡å°‘ `max_tokens` é™åˆ¶

### Q4: å†…å­˜å ç”¨è¿‡é«˜ï¼Ÿ

**A**: è§£å†³æ–¹æ¡ˆï¼š
1. å‡å°‘ `MAX_EMBEDDINGS_CACHE` å¤§å°
2. é™ä½ `GC_THRESHOLD` å€¼
3. å‡å°æ‰¹æ¬¡å¤§å° `batch_size`
4. ç¦ç”¨ç›¸ä¼¼åº¦æ£€æŸ¥

### Q5: å¦‚ä½•ç”Ÿæˆç‰¹å®šæ¯”ä¾‹çš„è¿è§„æ ·æœ¬ï¼Ÿ

**A**: ä¿®æ”¹é…ç½®ï¼š
```yaml
# main_config.yaml
quality:
  violation_ratio: 0.5  # 50%è¿è§„æ ·æœ¬
```

### Q6: æ”¯æŒå“ªäº›LLMæä¾›å•†ï¼Ÿ

**A**: å½“å‰æ”¯æŒï¼š
- OpenAI (GPTç³»åˆ—)
- é˜¿é‡Œäº‘DashScope (é€šä¹‰åƒé—®)
- ä»»ä½•å…¼å®¹OpenAI APIæ ¼å¼çš„æœåŠ¡

å¯é€šè¿‡å®ç° `LLMProvider` æ¥å£æ‰©å±•å…¶ä»–æä¾›å•†ã€‚

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
sample_generator/
â”œâ”€â”€ config/                    # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ main_config.yaml       # ä¸»é…ç½®
â”‚   â”œâ”€â”€ core_business.yaml     # ä¸šåŠ¡åœºæ™¯
â”‚   â”œâ”€â”€ violation_types.yaml   # è¿è§„ç±»å‹
â”‚   â”œâ”€â”€ user_profiles.yaml     # ç”¨æˆ·ç”»åƒ
â”‚   â”œâ”€â”€ dialogue_styles.yaml   # å¯¹è¯é£æ ¼
â”‚   â””â”€â”€ quality_rules.yaml     # è´¨é‡è§„åˆ™
â”œâ”€â”€ src/                       # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py            # ä¸»æµæ°´çº¿
â”‚   â”œâ”€â”€ config_manager.py      # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ variable_manager.py    # å˜é‡ç®¡ç†
â”‚   â”œâ”€â”€ prompt_generator.py    # Promptç”Ÿæˆ
â”‚   â”œâ”€â”€ llm_client.py          # LLMå®¢æˆ·ç«¯
â”‚   â”œâ”€â”€ quality_controller.py  # è´¨é‡æ§åˆ¶
â”‚   â”œâ”€â”€ logger_config.py       # æ—¥å¿—é…ç½®
â”‚   â”œâ”€â”€ constants.py           # å¸¸é‡å®šä¹‰
â”‚   â””â”€â”€ utils/                 # å·¥å…·ç±»
â”‚       â”œâ”€â”€ env_resolver.py    # ç¯å¢ƒå˜é‡è§£æ
â”‚       â””â”€â”€ key_mapper.py      # é”®åæ˜ å°„
â”œâ”€â”€ providers/                 # LLMæä¾›å•†é€‚é…å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # åŸºç±»
â”‚   â”œâ”€â”€ openai_provider.py     # OpenAIé€‚é…å™¨
â”‚   â””â”€â”€ dashscope_provider.py  # DashScopeé€‚é…å™¨
â”œâ”€â”€ sinks/                     # æ•°æ®è¾“å‡ºé€‚é…å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # åŸºç±»
â”‚   â””â”€â”€ jsonl_sink.py          # JSONLæ–‡ä»¶è¾“å‡º
â”œâ”€â”€ outputs/                   # ç”Ÿæˆç»“æœç›®å½•
â”œâ”€â”€ logs/                      # æ—¥å¿—æ–‡ä»¶ç›®å½•
â”œâ”€â”€ main.py                    # ç¨‹åºå…¥å£
â”œâ”€â”€ requirements.txt           # ä¾èµ–æ¸…å•
â”œâ”€â”€ .env.example               # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
```


---

## ğŸ” ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨

```python
import asyncio
from src.pipeline import GenerationPipeline

async def main():
    # åˆ›å»ºæµæ°´çº¿
    pipeline = GenerationPipeline("config")
    
    # ç”Ÿæˆ100ä¸ªæ ·æœ¬
    await pipeline.generate_samples_async(
        num_samples=100,
        output_path="outputs/samples.jsonl"
    )

if __name__ == "__main__":
    asyncio.run(main())
```

### ç¤ºä¾‹2: è‡ªå®šä¹‰é…ç½®

```python
from src.config_manager import ConfigManager
from src.pipeline import GenerationPipeline

# åŠ è½½è‡ªå®šä¹‰é…ç½®
config_manager = ConfigManager("my_config")
configs = config_manager.load_all_configs()

# ä¿®æ”¹é…ç½®
configs["main_config"]["generation"]["batch_size"] = 100
configs["main_config"]["quality"]["violation_ratio"] = 0.5

# åˆ›å»ºæµæ°´çº¿
pipeline = GenerationPipeline("my_config")
```

### ç¤ºä¾‹3: å•ç‹¬ä½¿ç”¨ç»„ä»¶

```python
from src.variable_manager import VariableManager
from src.prompt_generator import PromptGenerator
from src.config_manager import ConfigManager

# åŠ è½½é…ç½®
config_manager = ConfigManager("config")
configs = config_manager.load_all_configs()

# ä½¿ç”¨å˜é‡ç®¡ç†å™¨
var_manager = VariableManager(configs)
variables = var_manager.select_variables()
print(f"é€‰ä¸­çš„å˜é‡: {variables}")

# ä½¿ç”¨Promptç”Ÿæˆå™¨
prompt_gen = PromptGenerator(configs)
meta_prompt, vars = prompt_gen.generate_meta_prompt()
print(f"ç”Ÿæˆçš„Prompté•¿åº¦: {len(meta_prompt)}")
```

### ç¤ºä¾‹4: æ‰¹é‡å¤„ç†

```python
import asyncio
from src.llm_client import LLMClient

async def batch_generate():
    client = LLMClient(
        api_key="your-api-key",
        model="gpt-3.5-turbo",
        base_url="https://api.openai.com/v1/chat/completions"
    )
    
    prompts = [
        "ç”Ÿæˆä¸€ä¸ªé“¶è¡Œå¼€æˆ·å¯¹è¯",
        "ç”Ÿæˆä¸€ä¸ªè´·æ¬¾å’¨è¯¢å¯¹è¯",
        "ç”Ÿæˆä¸€ä¸ªæŠ•è¯‰å¤„ç†å¯¹è¯"
    ]
    
    results = await client.generate_texts_async(prompts)
    
    for i, result in enumerate(results):
        print(f"ç»“æœ {i+1}: {result[:100]}...")

asyncio.run(batch_generate())
```

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

### æµ‹è¯•ç¯å¢ƒ

- CPU: Intel i7-10700K
- å†…å­˜: 32GB
- ç½‘ç»œ: 100Mbps
- LLM: gpt-4.1-mini
- å¹¶å‘æ•°: 5

### æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | æ•°å€¼ |
|-----|------|
| å¹³å‡ç”Ÿæˆé€Ÿåº¦ | 0.8-1.2 æ ·æœ¬/ç§’ |
| æˆåŠŸç‡ | 92-96% |
| å†…å­˜å ç”¨ | 500MB-1GB |
| CPUå ç”¨ | 10-20% |
| å»é‡å‡†ç¡®ç‡ | >95% |

### å¤§è§„æ¨¡ç”Ÿæˆæµ‹è¯•

| æ ·æœ¬æ•° | è€—æ—¶ | æˆåŠŸç‡ | å¹³å‡é€Ÿåº¦ |
|-------|------|--------|---------|
| 100 | 2åˆ†é’Ÿ | 95% | 0.83/s |
| 1,000 | 18åˆ†é’Ÿ | 94% | 0.93/s |
| 10,000 | 3å°æ—¶ | 93% | 0.87/s |

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®ï¼

### å¼€å‘æµç¨‹

1. Forkæœ¬é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯Pull Request

### ä»£ç è§„èŒƒ

- éµå¾ªPEP 8ç¼–ç è§„èŒƒ
- æ·»åŠ å¿…è¦çš„æ–‡æ¡£å­—ç¬¦ä¸²
- ç¼–å†™å•å…ƒæµ‹è¯•
- æ›´æ–°ç›¸å…³æ–‡æ¡£

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ“® è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µ: [GitHub Repository]
- é—®é¢˜åé¦ˆ: [Issues]
- é‚®ç®±: [your-email@example.com]

---

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š

- [OpenAI](https://openai.com/) - GPTæ¨¡å‹
- [Sentence-Transformers](https://www.sbert.net/) - è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—
- [aiohttp](https://docs.aiohttp.org/) - å¼‚æ­¥HTTPå®¢æˆ·ç«¯
- [PyYAML](https://pyyaml.org/) - YAMLè§£æ

---

## ğŸ“š å‚è€ƒèµ„æ–™

- [é“¶è¡Œä¸šæ¶ˆè´¹è€…æƒç›Šä¿æŠ¤å·¥ä½œæŒ‡å¼•](https://www.cbirc.gov.cn/)
- [å…³äºè¿›ä¸€æ­¥è§„èŒƒé‡‘èè¥é”€å®£ä¼ è¡Œä¸ºçš„é€šçŸ¥](https://www.gov.cn/)
- [å•†ä¸šé“¶è¡Œç†è´¢ä¸šåŠ¡ç›‘ç£ç®¡ç†åŠæ³•](https://www.cbirc.gov.cn/)
- [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs/)
- [é˜¿é‡Œäº‘DashScopeæ–‡æ¡£](https://help.aliyun.com/zh/dashscope/)

---

**æœ€åæ›´æ–°**: 2025-12-11  
**ç‰ˆæœ¬**: 1.0.0  
**çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª
